{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit card fraud detection is a crucial task for financial institutions to minimize financial losses and protect their customers. Machine learning algorithms have proven to be effective tools for identifying fraudulent transactions due to their ability to learn complex patterns from large datasets. \n",
    "\n",
    "This project aims to develop a credit card fraud detection system using various machine learning algorithms:\n",
    "- Decision Trees.\n",
    "- Random Forest.\n",
    "- Light Gradient Boosting Machine (LightGBM).\n",
    "- XGBoost.\n",
    "- Artificial Neural Networks (ANN). \n",
    "- Isolation Forest\n",
    "- Local Outlier Factor (LOF).\n",
    "\n",
    "The project will utilize a comprehensive dataset of credit card transactions, including both fraudulent and legitimate transactions. The data will be preprocessed to handle missing values, outliers, and categorical variables. Subsequently, the machine learning algorithms will be trained and evaluated on the preprocessed data. The performance of each algorithm will be assessed using various metrics, such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "The project outcomes will provide insights into the effectiveness of different machine learning algorithms for credit card fraud detection. The findings can be used to guide the selection of appropriate algorithms for practical fraud detection systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(df.Class, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(counts, autopct='%1.2f%%', labels=labels)\n",
    "plt.legend(['Normal', 'Fraud'])\n",
    "plt.title('Type of transaction')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count= df['Class'].value_counts(normalize=False).sort_values()\n",
    "prop = df['Class'].value_counts(normalize=True)\n",
    "dist = pd.DataFrame({'Freq[N]':count,'Prop[%]':prop.round(4)})\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highly imbalanced dataset (99.83% normal, 0.17% fraudulent transactions) poses significant challenges for fraud detection models. This imbalance can lead to biased predictions, poor performance on the minority class, and misleading evaluation metrics. Models trained on such data may struggle to learn fraud patterns effectively, potentially missing critical fraudulent activities.\n",
    "\n",
    "Balancing the dataset is crucial to address these issues. It helps the model learn characteristics of both normal and fraudulent transactions equally, reducing bias and improving overall detection capabilities. Balanced data enables more meaningful model evaluation and aligns with the primary business objective of identifying fraud, even if it's rare. This approach leads to more robust and reliable fraud detection systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time and Amount Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_time = df.loc[df['Class'] == 0][\"Time\"]\n",
    "fraud_time = df.loc[df['Class'] == 1][\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "#Plot 1\n",
    "sns.boxplot(x='Class', y='Time', data=df, ax=axs[0])\n",
    "axs[0].set_title('Boxplot of Time by Class')\n",
    "axs[0].set_xlabel('Normal = 0, Fraud = 1')\n",
    "axs[0].set_ylabel('Time')\n",
    "\n",
    "# Plot 2\n",
    "sns.kdeplot(data=normal_time, ax=axs[1], label='Normal')\n",
    "sns.kdeplot(data=fraud_time, ax=axs[1], label='Fraud')\n",
    "axs[1].set_title('Density Plot of Time by Class')\n",
    "axs[1].set_xlabel('Time [s]')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_amount = df.loc[df['Class'] == 0]['Amount']\n",
    "fraud_amount = df.loc[df['Class'] == 1]['Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Plot 1\n",
    "sns.boxplot(x='Class', y='Amount', data=df, ax=axs[0])\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_title('Boxplot of Amount by Class (Log Scale)')\n",
    "axs[0].set_xlabel('Normal = 0, Fraud = 1')\n",
    "axs[0].set_ylabel('Amount (Log Scale)')\n",
    "\n",
    "# Plot 2\n",
    "sns.kdeplot(data=np.log1p(normal_amount), ax=axs[1], label='Normal', fill=True, color='blue')\n",
    "sns.kdeplot(data=np.log1p(fraud_amount), ax=axs[1], label='Fraud', fill=True, color='red')\n",
    "axs[1].set_title('Log-Transformed Density of Amount by Class')\n",
    "axs[1].set_xlabel('Log(Amount + 1)')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing transaction time and amount alone did not yield significant insights for distinguishing fraudulent transactions from normal ones. The distribution of transaction times for both normal and fraudulent transactions appeared to be similar, indicating that fraudulent transactions were not concentrated at specific times. Similarly, the amount spent on both normal and fraudulent transactions exhibited overlapping distributions, suggesting that there was no clear spending threshold that could be used to identify fraudulent activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, vmin=-1, vmax=1, cmap='vlag')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_correlations = pd.concat([correlation_matrix.unstack().sort_values(ascending=False).drop_duplicates().head(6),\n",
    "                              correlation_matrix.unstack().sort_values(ascending=True).drop_duplicates().head(5)])\n",
    "top_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_normalized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_resampled.shape)\n",
    "print(y_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modeling algorithms will be used for the fraud detection:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Light Gradient Boosting\n",
    "- XGB\n",
    "- Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search Parameters\n",
    "param_dist_dt = {\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search_dt = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                      param_distributions=param_dist_dt,\n",
    "                                      n_iter=10,  # Número de iteraciones\n",
    "                                      scoring='accuracy',\n",
    "                                      cv=5,\n",
    "                                      verbose=0,\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=42)\n",
    "\n",
    "random_search_dt.fit(X_train_resampled, y_train_resampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for Decision Tree\n",
    "best_dt = random_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test_normalized)\n",
    "\n",
    "# Results\n",
    "print(f'Best parameters found: {random_search_dt.best_params_}')\n",
    "print(f'Best cross-validation accuracy: {random_search_dt.best_score_:.4f}')\n",
    "\n",
    "test_accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f'Test set accuracy: {test_accuracy_dt:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para Randomized Search\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Ajuste al espacio de búsqueda correcto\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search_rf = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                                      param_distributions=param_dist_rf,\n",
    "                                      n_iter=10,  # Número de iteraciones\n",
    "                                      scoring='accuracy',\n",
    "                                      cv=5,\n",
    "                                      verbose=0,\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=42)\n",
    "\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for Random Forest\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test_normalized)\n",
    "\n",
    "# Results\n",
    "print(f'Best parameters found: {random_search_rf.best_params_}')\n",
    "print(f'Best cross-validation accuracy: {random_search_rf.best_score_:.4f}')\n",
    "\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Test set accuracy: {test_accuracy_rf:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingClassifier(),\n",
    "    param_distributions=param_dist_gb,\n",
    "    n_iter=10,  # Number of random parameter combinations to try\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Getting the best model and making predictions\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "y_pred_gb = best_gb.predict(X_test_normalized)\n",
    "\n",
    "# Printing results\n",
    "print(f'Best parameters found: {random_search_gb.best_params_}')\n",
    "print(f'Best cross-validation accuracy: {random_search_gb.best_score_:.4f}')\n",
    "\n",
    "# Calculating accuracy on the test set\n",
    "test_accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f'Test set accuracy: {test_accuracy_gb:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.1, 1, 5]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator=XGBClassifier(),\n",
    "                               param_grid=param_grid_xgb,\n",
    "                               scoring='accuracy',\n",
    "                               cv=5,\n",
    "                               verbose=0,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "# Ejecutar la búsqueda de cuadrícula en los datos resampleados\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Best parameters for XGB\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test_normalized)\n",
    "\n",
    "#Results\n",
    "print(f'Best parameters found: {grid_search_xgb.best_params_}')\n",
    "print(f'Best cross-validation accuracy: {grid_search_xgb.best_score_:.4f}')\n",
    "\n",
    "test_accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f'Test set accuracy: {test_accuracy_gb:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_ann.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model_ann.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_ann.fit(X_train_resampled, y_train_resampled,\n",
    "                        epochs=50, batch_size=64, verbose=1,\n",
    "                        validation_data=(X_test_normalized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ann = model_ann.predict(X_test_normalized).flatten()\n",
    "y_pred_ann_int = y_pred_ann.astype(np.int64)\n",
    "\n",
    "\n",
    "test_accuracy_ann = accuracy_score(y_test, y_pred_ann_int)\n",
    "print(f'Test set accuracy: {test_accuracy_ann:.4f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fraud Detection",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
